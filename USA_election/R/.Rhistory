# Selecione apenas as variáveis categóricas
df <- anamnese$categoria
# Converta o vetor para fator
df <- as.factor(df)
df
# Realize a clusterização com K-Modes
kmeans_model <- kmodes(data.frame(df), centers = 2)
# Visualize os clusters
print(kmeans_model)
# Realize a clusterização com K-Modes
kmeans_model <- kmodes(data.frame(df), centers = 2)
# Realize a clusterização com K-Modes
kmeans_model <- kmodes(data.frame(df), k = 2)
# Selecione apenas as variáveis categóricas
df <- data.frame(categoria = anamnese$categoria)
# Converta a coluna categoria para fator
df$categoria <- as.factor(df$categoria)
# Realize a clusterização com K-Modes
kmeans_model <- kmodes(df, k = 2)
# Visualize os clusters
print(kmeans_model)
# Realize a clusterização com K-Modes
kmeans_model <- kmodes(df, k = 2)
# Instale a biblioteca clustMixType
install.packages("clustMixType")
# Carregue a biblioteca clustMixType
library(clustMixType)
# Selecione apenas as variáveis categóricas
df <- data.frame(categoria = anamnese$categoria)
# Converta a coluna categoria para fator
df$categoria <- as.factor(df$categoria)
# Realize a clusterização com K-Prototypes
kprototypes_model <- kprototypes(df, k = 2)
# Instale a biblioteca cluster
install.packages("cluster")
# Carregue a biblioteca cluster
library(cluster)
# Selecione apenas as variáveis categóricas
df <- data.frame(categoria = anamnese$categoria)
# Converta a coluna categoria para fator
df$categoria <- as.factor(df$categoria)
# Calcule a distância entre os objetos
distancia <- daisy(df, metric = "gower")
# Realize a clusterização com PAMK
pamk_model <- pamk(distancia, k = 2)
# Realize a clusterização com PAMK
pamk_model <- pamk(distancia, k = 2)
# Calcule a distância entre os objetos
distancia <- daisy(df, metric = "gower")
distancia
pamk_model <- pamk(distancia, k = 2)
# Calcule a distância entre os objetos
distancia <- daisy(df, metric = "gower")
# Realize a clusterização com PAM
pam_model <- pam(distancia, k = 2)
# Visualize os clusters
print(pam_model)
# Visualize os clusters com ggplot
library(ggplot2)
ggplot(data.frame(df, cluster = pam_model$cluster), aes(x = categoria, fill = factor(cluster))) +
geom_bar() +
labs(fill = "Cluster")
library(cluster)
library(factoextra)
install.packages("factoextra")
library(factoextra)
dendrograma <- hclust(distancia, method = "ward.D2")
fviz_dendrogram(dendrograma, rect.hclust = TRUE)
dendrograma
fviz_dend(dendrograma, rect.hclust = TRUE)
fviz_dend(dendrograma)
dendrograma <- hclust(distancia, method = "ward.D2")
plot(dendograma)
plot(dendrograma)
# Carregue as bibliotecas necessárias
library(cluster)
library(factoextra)
library(ggplot2)
# Crie um dataframe com dados aleatórios
set.seed(123)
df <- data.frame(
x = rnorm(100, mean = 0, sd = 1),
y = rnorm(100, mean = 0, sd = 1)
)
# Realize a clusterização com K-Means
kmeans_model <- kmeans(df, centers = 3)
# Visualize os clusters
fviz_cluster(kmeans_model, data = df)
# Visualize os clusters com ggplot
ggplot(df, aes(x = x, y = y, color = factor(kmeans_model$cluster))) +
geom_point() +
labs(color = "Cluster") +
theme_classic()
# Carregue as bibliotecas necessárias
library(cluster)
library(factoextra)
library(ggplot2)
# Crie um dataframe com dados aleatórios
set.seed(123)
df <- data.frame(
x = rnorm(100, mean = 0, sd = 1),
y = rnorm(100, mean = 0, sd = 1)
)
# Calcule a distância entre os objetos
distancia <- dist(df)
# Realize a clusterização hierárquica
hclust_model <- hclust(distancia, method = "ward.D2")
# Visualize o dendrograma
fviz_dendrogram(hclust_model, rect.hclust = TRUE)
plot(hclust_model)
# Carregue as bibliotecas
library(tm)
library(cluster)
# Crie um corpus de textos
corpus <- Corpus(VectorSource(anamnese$content))
# Limpe o corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("portuguese"))
# Crie uma matriz de frequência de palavras
dtm <- DocumentTermMatrix(corpus)
dtm
anamnese
# Crie um vetor de strings com as anamneses
pacientes <- c(
"Paciente 1: 40 anos, hipertensão, diabetes, consulta inicial em 2022-01-01",
"Paciente 2: 30 anos, asma, exame de sangue em 2022-02-01",
"Paciente 3: 50 anos, doença cardíaca, tratamento medicamentoso desde 2022-03-01"
)
# Crie um dataframe com os dados de anamnese
anamnese <- data.frame(
id = 1:3,
start = c("2022-01-01", "2022-02-01", "2022-03-01"),
end = c("2022-01-31", "2022-02-28", "2022-03-31"),
content = pacientes,
categoria = c("Consulta", "Exame", "Tratamento")
)
# Crie um corpus de textos
corpus <- Corpus(VectorSource(anamnese$content))
# Limpe o corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("portuguese"))
# Crie uma matriz de frequência de palavras
dtm <- DocumentTermMatrix(corpus)
# Realize a clusterização
distancia <- dist(dtm)
hclust_model <- hclust(distancia, method = "ward.D2")
hclust_model
plot(hclust_model)
# Visualize o dendrograma
fviz_dendrogram(hclust_model, rect.hclust = TRUE)
# Instale as bibliotecas necessárias
install.packages("tidytext")
# Carregue as bibliotecas
library(tidytext)
library(cluster)
# Crie um dataframe de textos
df <- anamnese %>%
mutate(text = content) %>%
unnest_tokens(word, text)
library(tidyverse)
# Crie um dataframe de textos
df <- anamnese %>%
mutate(text = content) %>%
unnest_tokens(word, text)
# Limpe o dataframe
df <- df %>%
filter(!word %in% stopwords("portuguese"))
# Crie uma matriz de frequência de palavras
dtm <- df %>%
count(word) %>%
spread(word, n)
dtm
# Realize a clusterização
distancia <- dist(dtm)
hclust_model <- hclust(distancia, method = "ward.D2")
# Visualize o dendrograma
fviz_dendrogram(hclust_model, rect.hclust = TRUE)
hclust_model <- hclust(distancia, method = "ward.D2")
distancia <- dist(dtm, method = "euclidean")
distancia
class(distancia)
hclust_model <- hclust(distancia, method = "ward.D2")
dtm_matrix <- as.matrix(dtm)
distancia <- dist(dtm_matrix, method = "euclidean")
hclust_model <- hclust(distancia, method = "ward.D2")
# Crie uma matriz de frequência de palavras
dtm <- DocumentTermMatrix(corpus)
dtm
# Transforme o objeto dtm em uma matriz
dtm_matrix <- as.matrix(dtm)
# Calcule a distância euclidiana
distancia <- dist(dtm_matrix, method = "euclidean")
# Realize a clusterização
hclust_model <- hclust(distancia, method = "ward.D2")
hclust_model
# Visualize o dendrograma
fviz_dendrogram(hclust_model, rect.hclust = TRUE)
plot(hclust_model)
plot(hclust_model)
plot(hclust_model)
hclust_model
distancia
dtm_matrix
dtm
hclust_model
hclust_model <- hclust(distancia, method = "ward.D2")
plot(hclust_model)
dtm
dtm_matrix <- as.matrix(dtm)
dtm_matrix
# Carregue as bibliotecas
library(tidytext)
library(igraph)
library(ggraph)
install.packages("ggraph")
library(igraph)
library(ggraph)
# Crie um dataframe de textos
df <- anamnese %>%
mutate(text = content) %>%
unnest_tokens(word, text)
df
# Limpe o dataframe
df <- df %>%
filter(!word %in% stopwords("portuguese"))
df
# Crie uma rede de palavras
rede <- df %>%
group_by(word) %>%
summarise(freq = n()) %>%
arrange(desc(freq))
rede
# Crie um grafo
grafo <- rede %>%
graph_from_data_frame()
grafo
# Visualize o grafo
ggraph(grafo, layout = "fr") +
geom_node_point() +
geom_edge_link() +
theme_void()
# Carregue as bibliotecas
library(tidytext)
library(ggplot2)
# Crie um dataframe de textos
df <- anamnese %>%
mutate(text = content) %>%
unnest_tokens(word, text)
# Limpe o dataframe
df <- df %>%
filter(!word %in% stopwords("portuguese"))
# Crie uma matriz de frequência
matriz <- df %>%
group_by(word) %>%
summarise(freq = n()) %>%
arrange(desc(freq))
# Visualize o mapa de calor
ggplot(matriz, aes(x = word, y = freq, fill = freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
theme_void()
# Carregue as bibliotecas
library(tidytext)
library(ggplot2)
# Crie um dataframe de textos
df <- anamnese %>%
mutate(text = content) %>%
unnest_tokens(word, text)
# Limpe o dataframe
df <- df %>%
filter(!word %in% stopwords("portuguese"))
# Crie uma matriz de frequência
matriz <- df %>%
group_by(word) %>%
summarise(freq = n()) %>%
arrange(desc(freq))
# Realize o PCA
pca <- prcomp(matriz, scale = TRUE)
pca
# Realize o PCA
pca <- prcomp(matriz, scale = TRUE)
# Crie uma matriz de frequência
matriz <- df %>%
group_by(word) %>%
summarise(freq = n()) %>%
arrange(desc(freq))
matriz
# Realize o PCA
pca <- prcomp(matriz, scale = TRUE)
pca
# Visualize os componentes principais
ggplot(data.frame(PC1 = pca$x[, 1], PC2 = pca$x[, 2]), aes(x = PC1, y = PC2)) +
geom_point() +
theme_void()
# Realize o PCA
pca <- prcomp(matriz, scale = TRUE)
# Crie uma matriz de frequência de palavras
matriz <- df %>%
group_by(documento, word) %>%
summarise(freq = n()) %>%
spread(word, freq) %>%
replace((link unavailable)(.), 0)
# Crie uma matriz de frequência de palavras
matriz <- df %>%
group_by(documento, word) %>%
summarise(freq = n()) %>%
spread(word, freq) %>%
replace((link unavailable)(.), 0)
df %>%
group_by(documento, word)
# Crie um dataframe de textos
df <- anamnese %>%
mutate(text = content) %>%
unnest_tokens(word, text)
# Limpe o dataframe
df <- df %>%
filter(!word %in% stopwords("portuguese"))
# Crie uma matriz de frequência de palavras
matriz <- df %>%
group_by(documento, word) %>%
summarise(freq = n()) %>%
spread(word, freq) %>%
replace((link unavailable)(.), 0)
# Crie uma matriz de frequência de palavras
matriz <- df %>%
pivot_wider(id_cols = documento, names_from = word, values_from = freq, values_fill = 0)
# Crie um dataframe de textos
df <- anamnese %>%
mutate(text = content) %>%
unnest_tokens(word, text)
# Limpe o dataframe
df <- df %>%
filter(!word %in% stopwords("portuguese"))
df
# Crie uma matriz de frequência de palavras
matriz <- df %>%
pivot_wider(id_cols = documento, names_from = word, values_from = freq, values_fill = 0)
df <- anamnese %>%
mutate(text = content) %>%
unnest_tokens(word, text)
# Limpe o dataframe
df <- df %>%
filter(!word %in% stopwords("portuguese"))
# Adicione uma coluna de frequência
df <- df %>%
group_by(documento, word) %>%
summarise(freq = n())
df <- anamnese %>%
mutate(text = content) %>%
unnest_tokens(word, text)
df
# Limpe o dataframe
df <- df %>%
filter(!word %in% stopwords("portuguese"))
# Limpe o dataframe
df <- df %>%
filter(!word %in% stopwords("portuguese"))
df
# Adicione uma coluna de frequência
df <- df %>%
group_by(documento, word) %>%
summarise(freq = n())
# Crie uma matriz de frequência de palavras
matriz <- df %>%
pivot_wider(id_cols = documento, names_from = word, values_from = freq, values_fill = 0)
# Crie um dataframe de textos
# Carregue as bibliotecas
library(tidytext)
library(tidyr)
library(ggplot2)
# Crie um dataframe de textos
df <- anamnese %>%
mutate(text = content) %>%
unnest_tokens(word, text)
# Limpe o dataframe
df <- df %>%
filter(!word %in% stopwords("portuguese"))
# Adicione uma coluna de frequência
df <- df %>%
group_by(documento, word) %>%
summarise(freq = n())
pacientes <- c(
"Paciente 1: 40 anos, hipertensão, diabetes, consulta inicial em 2022-01-01",
"Paciente 2: 30 anos, asma, exame de sangue em 2022-02-01",
"Paciente 3: 50 anos, doença cardíaca, tratamento medicamentoso desde 2022-03-01"
)
# Crie um dataframe com os dados de anamnese
anamnese <- data.frame(
id = 1:3,
start = c("2022-01-01", "2022-02-01", "2022-03-01"),
end = c("2022-01-31", "2022-02-28", "2022-03-31"),
content = pacientes,
categoria = c("Consulta", "Exame", "Tratamento")
)
# Carregue as bibliotecas
library(tidytext)
library(ggplot2)
# Crie um dataframe de textos
df <- anamnese %>%
mutate(text = content) %>%
unnest_tokens(word, text)
# Limpe o dataframe
df <- df %>%
filter(!word %in% stopwords("portuguese"))
# Crie uma matriz de frequência
matriz <- df %>%
group_by(word) %>%
summarise(freq = n()) %>%
arrange(desc(freq))
# Visualize o mapa de calor
ggplot(matriz, aes(x = word, y = freq, fill = freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
theme_void()
# Carregue as bibliotecas
library(tidytext)
library(ggplot2)
# Crie um dataframe de textos
# Carregue as bibliotecas
library(tidytext)
library(tidyr)
library(ggplot2)
# Crie um dataframe de textos
df <- anamnese %>%
mutate(text = content) %>%
unnest_tokens(word, text)
# Limpe o dataframe
df <- df %>%
filter(!word %in% stopwords("portuguese"))
# Adicione uma coluna de frequência
df <- df %>%
group_by(documento, word) %>%
summarise(freq = n())
# Crie um dataframe de textos
df <- anamnese %>%
mutate(documento = row_number(),  # Crie uma coluna "documento"
text = content) %>%
unnest_tokens(word, text)
# Limpe o dataframe
df <- df %>%
filter(!word %in% stopwords("portuguese"))
# Adicione uma coluna de frequência
df <- df %>%
group_by(documento, word) %>%
summarise(freq = n())
# Crie uma matriz de frequência de palavras
matriz <- df %>%
pivot_wider(id_cols = documento, names_from = word, values_from = freq, values_fill = 0)
# Realize o PCA
pca <- prcomp(matriz[,-1], scale = TRUE)
# Visualize os componentes principais
ggplot(data.frame(PC1 = pca$x[, 1], PC2 = pca$x[, 2]), aes(x = PC1, y = PC2)) +
geom_point() +
theme_void()
# Visualize os componentes principais
ggplot(data.frame(PC1 = pca$x[, 1], PC2 = pca$x[, 2]), aes(x = PC1, y = PC2)) +
geom_point() +
theme_void()
# Realize o PCA
pca <- prcomp(matriz[,-1], scale = TRUE)
# Visualize os componentes principais
ggplot(data.frame(PC1 = pca$x[, 1], PC2 = pca$x[, 2]), aes(x = PC1, y = PC2)) +
geom_point() +
theme_void()
# Crie um dataframe de textos
df <- anamnese %>%
mutate(documento = row_number(),  # Crie uma coluna "documento"
text = content) %>%
unnest_tokens(word, text)
df
# Limpe o dataframe
df <- df %>%
filter(!word %in% stopwords("portuguese"))
df
# Adicione uma coluna de frequência
df <- df %>%
group_by(documento, word) %>%
summarise(freq = n())
df
# Crie uma matriz de frequência de palavras
matriz <- df %>%
pivot_wider(id_cols = documento, names_from = word, values_from = freq, values_fill = 0)
matriz
# Realize o PCA
pca <- prcomp(matriz[,-1], scale = TRUE)
# Crie um dataframe de textos
df <- anamnese %>%
mutate(documento = row_number(),  # Crie uma coluna "documento"
text = content) %>%
unnest_tokens(word, text)
# Limpe o dataframe
df <- df %>%
filter(!word %in% stopwords("portuguese"))
# Adicione uma coluna de frequência
df <- df %>%
group_by(documento, word) %>%
summarise(freq = n())
# Crie uma matriz de frequência de palavras
matriz <- df %>%
pivot_wider(id_cols = documento, names_from = word, values_from = freq, values_fill = 0)
# Remova colunas constantes ou com valores zero
matriz <- matriz[, apply(matriz, 2, function(x) var(x, na.rm = TRUE) > 0)]
# Realize o PCA
pca <- prcomp(matriz, scale = TRUE)
# Visualize os componentes principais
ggplot(data.frame(PC1 = pca$x[, 1], PC2 = pca$x[, 2]), aes(x = PC1, y = PC2)) +
geom_point() +
theme_void()
